{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = 'input/inchi-preprocess'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "if not os.path.exists('input/pytorch-image-models'):\n",
    "    os.makedirs('input/pytorch-image-models')\n",
    "    \n",
    "if not os.path.exists('input/pytorch-image-models/pytorch-image-models-master'):\n",
    "    os.makedirs('input/pytorch-image-models/pytorch-image-models-master')\n",
    "    \n",
    "if not os.path.exists('input/inchi-resnet-lstm-with-attention-starter'):\n",
    "    os.makedirs('input/inchi-resnet-lstm-with-attention-starter')\n",
    "    \n",
    "if not os.path.isfile('train_labels.csv'):\n",
    "    !wget https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/22422/2153105/compressed/train_labels.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1619975509&Signature=YuzUw%2FIcDEdnraZ0vyNS7q5q%2BT3c7DcBv8hV%2F%2FC0vqtYscLpal9ZBczcIPNamavvLbYQaMInivBGChSHO9ld1%2Bc6Wix5DnHAhWAo6EIimiIbk3Nbu%2F3%2BuU5H7pmZPwYiU0WHjviPmRrTbw24OctAYUbgrL9eSQGOgcgigQ88rI8wfPonZ4C%2Fk7fPizr6wtxDfRaG5TFe%2FkSNAf3NZvlydiAHlVknzozxaws%2FsW9I%2BTkPrJzNrjBUIz61Ync3EFaMAS2owdfGx7PDzHcKmYAHkAgBKTriO8Umd3bQrXJ56i%2FSpqZxNWzdEb1FceFKKOg8L%2BeP6lboxWkcbc38PBIx9A%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain_labels.csv.zip\n",
    "    raise 'Please unzip the contents of this archive and place the train_labels.csv file in the root directory!'\n",
    "    \n",
    "if not os.path.isfile('sample_submission.csv'):\n",
    "    !wget https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/22422/2153105/compressed/sample_submission.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1619975527&Signature=iaBM26uSwKKWMoLIG8Y3FZUFGccrp6C61Z5jz4HEqHruBPIzF7FeMtM3kNb7zN%2BIkp8bfOc4WMlvP%2FFApY1AkLUygtC9WiCW8QeQh5L5ob0HpIDAMa3cp3vXH6KRNTT80GnR6S67a%2FD%2B%2F11lUIWQibGb6jwkyE2hA5wuO3VpWvEd%2BV429kV3XizASQeD%2B2%2BGc2%2FjyQTu6ayAUnrVWluVTBAxdCKeKtsJ2%2B5d3kTkMZ6zqqSe30FKIyKrw1An4N7oBOYnsxy4CrqF4%2FcUasLCdGE6XLkfSmLj013Lh7LUkNOaKuk27FNce0%2Fq2Iw1lYc80P2pQAIKU0no9bVBHCWBdw%3D%3D&response-content-disposition=attachment%3B+filename%3Dsample_submission.csv.zip\n",
    "    raise 'Please unzip the contents of this archive and place the sample_submissions.csv file in the root directory!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import dill\n",
    "tqdm.pandas()\n",
    "import torch\n",
    "\n",
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('train_labels.csv')\n",
    "print(f'train.shape: {train.shape}')\n",
    "\n",
    "# ====================================================\n",
    "# Preprocess functions\n",
    "# ====================================================\n",
    "def split_form(form):\n",
    "    string = ''\n",
    "    for i in re.findall(r\"[A-Z][^A-Z]*\", form):\n",
    "        elem = re.match(r\"\\D+\", i).group()\n",
    "        num = i.replace(elem, \"\")\n",
    "        if num == \"\":\n",
    "            string += f\"{elem} \"\n",
    "        else:\n",
    "            string += f\"{elem} {str(num)} \"\n",
    "    return string.rstrip(' ')\n",
    "\n",
    "def split_form2(form):\n",
    "    string = ''\n",
    "    for i in re.findall(r\"[a-z][^a-z]*\", form):\n",
    "        elem = i[0]\n",
    "        num = i.replace(elem, \"\").replace('/', \"\")\n",
    "        num_string = ''\n",
    "        for j in re.findall(r\"[0-9]+[^0-9]*\", num):\n",
    "            num_list = list(re.findall(r'\\d+', j))\n",
    "            assert len(num_list) == 1, f\"len(num_list) != 1\"\n",
    "            _num = num_list[0]\n",
    "            if j == _num:\n",
    "                num_string += f\"{_num} \"\n",
    "            else:\n",
    "                extra = j.replace(_num, \"\")\n",
    "                num_string += f\"{_num} {' '.join(list(extra))} \"\n",
    "        string += f\"/{elem} {num_string}\"\n",
    "    return string.rstrip(' ')\n",
    "\n",
    "# ====================================================\n",
    "# Tokenizer\n",
    "# ====================================================\n",
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            vocab.update(text.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for i, s in enumerate(vocab):\n",
    "            self.stoi[s] = i\n",
    "        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n",
    "        \n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        sequence.append(self.stoi['<sos>'])\n",
    "        for s in text.split(' '):\n",
    "            sequence.append(self.stoi[s])\n",
    "        sequence.append(self.stoi['<eos>'])\n",
    "        return sequence\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = self.text_to_sequence(text)\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = self.sequence_to_text(sequence)\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    def predict_caption(self, sequence):\n",
    "        caption = ''\n",
    "        for i in sequence:\n",
    "            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n",
    "                break\n",
    "            caption += self.itos[i]\n",
    "        return caption\n",
    "    \n",
    "    def predict_captions(self, sequences):\n",
    "        captions = []\n",
    "        for sequence in sequences:\n",
    "            caption = self.predict_caption(sequence)\n",
    "            captions.append(caption)\n",
    "        return captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    # ====================================================\n",
    "    # preprocess train.csv\n",
    "    # ====================================================\n",
    "    train['InChI_1'] = train['InChI'].progress_apply(lambda x: x.split('/')[1])\n",
    "    train['InChI_text'] = train['InChI_1'].progress_apply(split_form) + ' ' + \\\n",
    "                            train['InChI'].apply(lambda x: '/'.join(x.split('/')[2:])).progress_apply(split_form2).values\n",
    "    # ====================================================\n",
    "    # create tokenizer\n",
    "    # ====================================================\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train['InChI_text'].values)\n",
    "    torch.save(tokenizer, f\"{OUTPUT_DIR}/tokenizer.pth\", pickle_module=dill)\n",
    "    print('Saved tokenizer')\n",
    "    # ====================================================\n",
    "    # preprocess train.csv\n",
    "    # ====================================================\n",
    "    lengths = []\n",
    "    tk0 = tqdm(train['InChI_text'].values, total=len(train))\n",
    "    for text in tk0:\n",
    "        seq = tokenizer.text_to_sequence(text)\n",
    "        length = len(seq) - 2\n",
    "        lengths.append(length)\n",
    "    train['InChI_length'] = lengths\n",
    "    train.to_json(f\"{OUTPUT_DIR}/train.json\")\n",
    "    print('Saved preprocessed train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
