{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 0 ns (2021-04-30T00:18:11/2021-04-30T00:18:11)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 0 ns (2021-04-30T00:18:11/2021-04-30T00:18:11)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = 'input/inchi-preprocess'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "if not os.path.exists('input/pytorch-image-models'):\n",
    "    os.makedirs('input/pytorch-image-models')\n",
    "    \n",
    "if not os.path.exists('input/pytorch-image-models/pytorch-image-models-master'):\n",
    "    os.makedirs('input/pytorch-image-models/pytorch-image-models-master')\n",
    "    \n",
    "if not os.path.exists('input/inchi-resnet-lstm-with-attention-starter'):\n",
    "    os.makedirs('input/inchi-resnet-lstm-with-attention-starter')\n",
    "    \n",
    "if not os.path.isfile('train_labels.csv') or not os.path.isfile('sample_submission.csv'):\n",
    "    raise Exception('Please unzip the contents of this archive and place the train_labels.csv and sample_submission.csv file in the root directory! https://www.kaggle.com/c/bms-molecular-translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 5.94 s (2021-04-30T00:18:12/2021-04-30T00:18:18)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (2424186, 2)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import dill\n",
    "tqdm.pandas()\n",
    "import torch\n",
    "\n",
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('train_labels.csv')\n",
    "print(f'train.shape: {train.shape}')\n",
    "\n",
    "# ====================================================\n",
    "# Preprocess functions\n",
    "# ====================================================\n",
    "def split_form(form):\n",
    "    string = ''\n",
    "    for i in re.findall(r\"[A-Z][^A-Z]*\", form):\n",
    "        elem = re.match(r\"\\D+\", i).group()\n",
    "        num = i.replace(elem, \"\")\n",
    "        if num == \"\":\n",
    "            string += f\"{elem} \"\n",
    "        else:\n",
    "            string += f\"{elem} {str(num)} \"\n",
    "    return string.rstrip(' ')\n",
    "\n",
    "def split_form2(form):\n",
    "    string = ''\n",
    "    for i in re.findall(r\"[a-z][^a-z]*\", form):\n",
    "        elem = i[0]\n",
    "        num = i.replace(elem, \"\").replace('/', \"\")\n",
    "        num_string = ''\n",
    "        for j in re.findall(r\"[0-9]+[^0-9]*\", num):\n",
    "            num_list = list(re.findall(r'\\d+', j))\n",
    "            assert len(num_list) == 1, f\"len(num_list) != 1\"\n",
    "            _num = num_list[0]\n",
    "            if j == _num:\n",
    "                num_string += f\"{_num} \"\n",
    "            else:\n",
    "                extra = j.replace(_num, \"\")\n",
    "                num_string += f\"{_num} {' '.join(list(extra))} \"\n",
    "        string += f\"/{elem} {num_string}\"\n",
    "    return string.rstrip(' ')\n",
    "\n",
    "# ====================================================\n",
    "# Tokenizer\n",
    "# ====================================================\n",
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            vocab.update(text.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for i, s in enumerate(vocab):\n",
    "            self.stoi[s] = i\n",
    "        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n",
    "        \n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        sequence.append(self.stoi['<sos>'])\n",
    "        for s in text.split(' '):\n",
    "            sequence.append(self.stoi[s])\n",
    "        sequence.append(self.stoi['<eos>'])\n",
    "        return sequence\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = self.text_to_sequence(text)\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = self.sequence_to_text(sequence)\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    def predict_caption(self, sequence):\n",
    "        caption = ''\n",
    "        for i in sequence:\n",
    "            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n",
    "                break\n",
    "            caption += self.itos[i]\n",
    "        return caption\n",
    "    \n",
    "    def predict_captions(self, sequences):\n",
    "        captions = []\n",
    "        for sequence in sequences:\n",
    "            caption = self.predict_caption(sequence)\n",
    "            captions.append(caption)\n",
    "        return captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 0 ns (2021-04-30T00:18:18/2021-04-30T00:18:18)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    # ====================================================\n",
    "    # preprocess train.csv\n",
    "    # ====================================================\n",
    "    train['InChI_1'] = train['InChI'].progress_apply(lambda x: x.split('/')[1])\n",
    "    train['InChI_text'] = train['InChI_1'].progress_apply(split_form) + ' ' + \\\n",
    "                            train['InChI'].apply(lambda x: '/'.join(x.split('/')[2:])).progress_apply(split_form2).values\n",
    "    # ====================================================\n",
    "    # create tokenizer\n",
    "    # ====================================================\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train['InChI_text'].values)\n",
    "    torch.save(tokenizer, f\"{OUTPUT_DIR}/tokenizer.pth\", pickle_module=dill)\n",
    "    print('Saved tokenizer')\n",
    "    # ====================================================\n",
    "    # preprocess train.csv\n",
    "    # ====================================================\n",
    "    lengths = []\n",
    "    tk0 = tqdm(train['InChI_text'].values, total=len(train))\n",
    "    for text in tk0:\n",
    "        seq = tokenizer.text_to_sequence(text)\n",
    "        length = len(seq) - 2\n",
    "        lengths.append(length)\n",
    "    train['InChI_length'] = lengths\n",
    "    train.to_json(f\"{OUTPUT_DIR}/train.json\")\n",
    "    print('Saved preprocessed train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>⌛ 32.2 s (2021-04-30T00:18:18)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4c042e6f6b4a0a86e2ee2a742a3583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2424186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c193d1fdf149a192f706a6793e1428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2424186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab98226e27ea4846be620e35f9d05ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2424186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
